[2m2025-04-14T15:51:31.134585Z[0m [32m INFO[0m [2mtext_generation_launcher[0m[2m:[0m Args {
    model_id: "HuggingFaceH4/zephyr-7b-beta",
    revision: None,
    validation_workers: 2,
    sharded: None,
    num_shard: None,
    quantize: None,
    speculate: None,
    dtype: None,
    kv_cache_dtype: None,
    trust_remote_code: false,
    max_concurrent_requests: 128,
    max_best_of: 2,
    max_stop_sequences: 4,
    max_top_n_tokens: 5,
    max_input_tokens: None,
    max_input_length: None,
    max_total_tokens: None,
    waiting_served_ratio: 0.3,
    max_batch_prefill_tokens: None,
    max_batch_total_tokens: None,
    max_waiting_tokens: 20,
    max_batch_size: None,
    cuda_graphs: None,
    hostname: "r-jdelavande-dev-tgi-3fo3lmzg-ceec7-vdv2x",
    port: 3000,
    shard_uds_path: "/tmp/text-generation-server",
    master_addr: "localhost",
    master_port: 29500,
    huggingface_hub_cache: None,
    weights_cache_override: None,
    disable_custom_kernels: true,
    cuda_memory_fraction: 1.0,
    rope_scaling: None,
    rope_factor: None,
    json_output: false,
    otlp_endpoint: None,
    otlp_service_name: "text-generation-inference.router",
    cors_allow_origin: [],
    api_key: None,
    watermark_gamma: None,
    watermark_delta: None,
    ngrok: false,
    ngrok_authtoken: None,
    ngrok_edge: None,
    tokenizer_config_path: None,
    disable_grammar_support: false,
    env: false,
    max_client_batch_size: 4,
    lora_adapters: None,
    usage_stats: On,
    payload_limit: 2000000,
    enable_prefill_logprobs: false,
    graceful_termination_timeout: 90,
}
[2m2025-04-14T15:51:31.175673Z[0m [33m WARN[0m [2mtext_generation_launcher::gpu[0m[2m:[0m Cannot determine GPU compute capability: ModuleNotFoundError: No module named 'torch'
[2m2025-04-14T15:51:31.175693Z[0m [32m INFO[0m [2mtext_generation_launcher[0m[2m:[0m Using attention flashinfer - Prefix caching true
[2m2025-04-14T15:51:31.238823Z[0m [32m INFO[0m [2mtext_generation_launcher[0m[2m:[0m Default `max_batch_prefill_tokens` to 8049
[2m2025-04-14T15:51:31.238844Z[0m [32m INFO[0m [2mtext_generation_launcher[0m[2m:[0m Using default cuda graphs [1, 2, 4, 8, 16, 32]
[2m2025-04-14T15:51:31.238980Z[0m [32m INFO[0m [1mdownload[0m: [2mtext_generation_launcher[0m[2m:[0m Starting check and download process for HuggingFaceH4/zephyr-7b-beta
[2m2025-04-14T15:51:36.389275Z[0m [32m INFO[0m [2mtext_generation_launcher[0m[2m:[0m Files are already present on the host. Skipping download.
[2m2025-04-14T15:51:37.045490Z[0m [32m INFO[0m [1mdownload[0m: [2mtext_generation_launcher[0m[2m:[0m Successfully downloaded weights for HuggingFaceH4/zephyr-7b-beta
[2m2025-04-14T15:51:37.045706Z[0m [32m INFO[0m [1mshard-manager[0m: [2mtext_generation_launcher[0m[2m:[0m Starting shard [2m[3mrank[0m[2m=[0m0[0m
[2m2025-04-14T15:51:40.518114Z[0m [32m INFO[0m [2mtext_generation_launcher[0m[2m:[0m Using prefix caching = True
[2m2025-04-14T15:51:40.518184Z[0m [32m INFO[0m [2mtext_generation_launcher[0m[2m:[0m Using Attention = flashinfer
[2m2025-04-14T15:51:42.735528Z[0m [33m WARN[0m [2mtext_generation_launcher[0m[2m:[0m Could not import Flash Attention enabled models: Flash Attention V2 is not installed.
Use the official Docker image (ghcr.io/huggingface/text-generation-inference:latest) or install flash attention v2 with `cd server && make install install-flash-attention-v2-cuda`
[2m2025-04-14T15:51:42.736901Z[0m [33m WARN[0m [2mtext_generation_launcher[0m[2m:[0m Could not import Mamba: No module named 'mamba_ssm'
[2m2025-04-14T15:51:43.004591Z[0m [33m WARN[0m [2mtext_generation_launcher[0m[2m:[0m Could not import Flash Transformers Backend: Flash Attention V2 is not installed.
Use the official Docker image (ghcr.io/huggingface/text-generation-inference:latest) or install flash attention v2 with `cd server && make install install-flash-attention-v2-cuda`
[2m2025-04-14T15:51:47.056968Z[0m [32m INFO[0m [1mshard-manager[0m: [2mtext_generation_launcher[0m[2m:[0m Waiting for shard to be ready... [2m[3mrank[0m[2m=[0m0[0m
[2m2025-04-14T15:51:57.066093Z[0m [32m INFO[0m [1mshard-manager[0m: [2mtext_generation_launcher[0m[2m:[0m Waiting for shard to be ready... [2m[3mrank[0m[2m=[0m0[0m
[2m2025-04-14T15:51:57.325997Z[0m [32m INFO[0m [2mtext_generation_launcher[0m[2m:[0m Using prefill chunking = False
[2m2025-04-14T15:51:57.333336Z[0m [32m INFO[0m [2mtext_generation_launcher[0m[2m:[0m Server started at unix:///tmp/text-generation-server-0
[2m2025-04-14T15:51:57.366368Z[0m [32m INFO[0m [1mshard-manager[0m: [2mtext_generation_launcher[0m[2m:[0m Shard ready in 20.319623577s [2m[3mrank[0m[2m=[0m0[0m
[2m2025-04-14T15:51:57.464248Z[0m [32m INFO[0m [2mtext_generation_launcher[0m[2m:[0m Starting Webserver
[2m2025-04-14T15:51:57.522731Z[0m [32m INFO[0m [2mtext_generation_router_v3[0m[2m:[0m [2mbackends/v3/src/lib.rs[0m[2m:[0m[2m125:[0m Warming up model
[2m2025-04-14T15:52:02.308730Z[0m [33m WARN[0m [2mtext_generation_router_v3[0m[2m:[0m [2mbackends/v3/src/lib.rs[0m[2m:[0m[2m79:[0m Model does not support automatic max batch total tokens
[2m2025-04-14T15:52:02.308755Z[0m [32m INFO[0m [2mtext_generation_router_v3[0m[2m:[0m [2mbackends/v3/src/lib.rs[0m[2m:[0m[2m137:[0m Setting max batch total tokens to 16000
[2m2025-04-14T15:52:02.308802Z[0m [32m INFO[0m [2mtext_generation_router_v3[0m[2m:[0m [2mbackends/v3/src/lib.rs[0m[2m:[0m[2m166:[0m Using backend V3
[2m2025-04-14T15:52:02.308813Z[0m [32m INFO[0m [2mtext_generation_router[0m[2m:[0m [2mbackends/v3/src/main.rs[0m[2m:[0m[2m162:[0m Maximum input tokens defaulted to 8048
[2m2025-04-14T15:52:02.308820Z[0m [32m INFO[0m [2mtext_generation_router[0m[2m:[0m [2mbackends/v3/src/main.rs[0m[2m:[0m[2m168:[0m Maximum total tokens defaulted to 8049
[2m2025-04-14T15:52:02.308836Z[0m [32m INFO[0m [2mtext_generation_router::server[0m[2m:[0m [2mrouter/src/server.rs[0m[2m:[0m[2m1560:[0m Using the Hugging Face API
[2m2025-04-14T15:52:02.671188Z[0m [32m INFO[0m [2mtext_generation_router::server[0m[2m:[0m [2mrouter/src/server.rs[0m[2m:[0m[2m2309:[0m Serving revision 892b3d7a7b1cf10c7a701c60881cd93df615734c of model HuggingFaceH4/zephyr-7b-beta
[2m2025-04-14T15:52:02.671224Z[0m [33m WARN[0m [2mtext_generation_router::server[0m[2m:[0m [2mrouter/src/server.rs[0m[2m:[0m[2m1648:[0m Tokenizer_config None - Some("/home/user/.cache/huggingface/hub/models--HuggingFaceH4--zephyr-7b-beta/snapshots/892b3d7a7b1cf10c7a701c60881cd93df615734c/tokenizer_config.json")
[2m2025-04-14T15:52:02.679061Z[0m [31mERROR[0m [2mtext_generation_router::server[0m[2m:[0m [2mrouter/src/server.rs[0m[2m:[0m[2m1672:[0m Failed to import python tokenizer ModuleNotFoundError: No module named 'transformers'
[2m2025-04-14T15:52:02.679079Z[0m [33m WARN[0m [2mtext_generation_router::server[0m[2m:[0m [2mrouter/src/server.rs[0m[2m:[0m[2m1440:[0m Odd tokenizer detected, falling back on legacy tokenization
[2m2025-04-14T15:52:02.679164Z[0m [32m INFO[0m [2mtext_generation_router::server[0m[2m:[0m [2mrouter/src/server.rs[0m[2m:[0m[2m1716:[0m Using config Some(Mistral)
[2m2025-04-14T15:52:02.776832Z[0m [33m WARN[0m [2mtext_generation_router::server[0m[2m:[0m [2mrouter/src/server.rs[0m[2m:[0m[2m1879:[0m Invalid hostname, defaulting to 0.0.0.0
[2m2025-04-14T15:52:02.780106Z[0m [32m INFO[0m [2mtext_generation_router::server[0m[2m:[0m [2mrouter/src/server.rs[0m[2m:[0m[2m2266:[0m Connected
